name: FINAL - Deep Network URL Scraper

on:
  workflow_dispatch:
    inputs:
      page_url:
        description: 'URL of the page to scrape'
        required: true
        default: 'https://www.redditsoccerstreams.name/hd-10/'

jobs:
  scrape-url:
    runs-on: ubuntu-latest
    timeout-minutes: 5 # The job will time out after 5 minutes

    steps:
    - name: 1. Install Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends wget nodejs npm
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo apt-get install -y ./google-chrome-stable_current_amd64.deb || true
        rm google-chrome-stable_current_amd64.deb

    - name: 2. Scrape All Network URLs
      run: |
        # Install puppeteer and stealth plugins
        npm i puppeteer-extra puppeteer-extra-plugin-stealth

        # Create the advanced network scraper script
        cat <<'EOF' > scraper.js
        const puppeteer = require('puppeteer-extra');
        const StealthPlugin = require('puppeteer-extra-plugin-stealth');
        puppeteer.use(StealthPlugin());

        const PAGE_URL = process.env.PAGE_URL;
        let foundUrls = new Set();

        (async () => {
          console.log('üöÄ Launching stealth browser to sniff ALL network traffic...');
          const browser = await puppeteer.launch({
            headless: 'new',
            executablePath: '/usr/bin/google-chrome',
            args: ['--no-sandbox', '--disable-gpu', '--disable-dev-shm-usage']
          });

          const page = await browser.newPage();
          const client = await page.target().createCDPSession();
          await client.send('Network.enable');

          console.log('‚úÖ Network listener attached.');

          // Listen for all network requests
          client.on('Network.requestWillBeSent', (event) => {
            const url = event.request.url;
            // Log every unique URL that is not a data image
            if (!url.startsWith('data:image') && !foundUrls.has(url)) {
              foundUrls.add(url);
              console.log(`DETECTED URL: ${url}`);
            }
          });

          console.log(` Navigating to: ${PAGE_URL}`);
          await page.goto(PAGE_URL, { waitUntil: 'networkidle2', timeout: 60000 });

          console.log('‚úÖ Page loaded. Clicking body to trigger video player...');
          try {
            await new Promise(resolve => setTimeout(resolve, 5000));
            await page.click('body');
            console.log('‚úÖ Click sent.');
          } catch (error) {
            console.error('Could not click page, but listener is still active.');
          }
          
          console.log('‚è≥ Waiting for 60 seconds to capture all network requests...');
          await new Promise(resolve => setTimeout(resolve, 60000));

          console.log('‚úÖ Scraping finished. Closing browser.');
          await browser.close();

          if (foundUrls.size === 0) {
            console.error('‚ùå FAILED: No network URLs were detected.');
            process.exit(1);
          } else {
            process.exit(0);
          }
        })();
        EOF

        # Run the scraper script
        node scraper.js
      env:
        PAGE_URL: ${{ github.event.inputs.page_url }}
